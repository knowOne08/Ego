# This is a reference workflow for your BLOG repository
# Copy this file to your blog repo at: .github/workflows/sync-to-supabase.yml

name: Sync Blogs to Supabase

on:
  push:
    branches: [ main, master ]
    paths: 
      - 'blogs/**'
  workflow_dispatch: # Allow manual triggers
  schedule:
    - cron: '0 12 * * *' # Daily at noon (optional)

jobs:
  sync-blogs:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout blog repository
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        
    - name: Install dependencies
      run: |
        npm init -y
        npm install @supabase/supabase-js front-matter marked glob
        
    - name: Create sync script
      run: |
        cat > sync-blogs.js << 'EOF'
        const { createClient } = require('@supabase/supabase-js');
        const fs = require('fs').promises;
        const path = require('path');
        const { marked } = require('marked');
        const frontMatter = require('front-matter');
        const glob = require('glob');

        // Initialize Supabase client
        const supabase = createClient(
          process.env.SUPABASE_URL,
          process.env.SUPABASE_SERVICE_KEY
        );

        // GitHub raw content base URL for THIS blog repository
        const GITHUB_RAW_BASE = `https://raw.githubusercontent.com/${process.env.GITHUB_REPOSITORY}/main`;

        class GitHubBlogSync {
          constructor() {
            this.blogDir = 'blogs';
            this.processedBlogs = [];
            this.errors = [];
          }

          async syncAllBlogs() {
            try {
              console.log('üöÄ Starting blog sync from external repo...');
              
              const blogFiles = glob.sync(`${this.blogDir}/*/README.md`);
              console.log(`üìö Found ${blogFiles.length} blog(s) to process`);

              for (const filePath of blogFiles) {
                try {
                  await this.processBlog(filePath);
                } catch (error) {
                  console.error(`‚ùå Error processing ${filePath}:`, error.message);
                  this.errors.push({ file: filePath, error: error.message });
                }
              }

              console.log(`‚úÖ Successfully processed ${this.processedBlogs.length} blogs`);
              if (this.errors.length > 0) {
                console.log(`‚ö†Ô∏è  ${this.errors.length} errors occurred`);
                this.errors.forEach(err => console.log(`   - ${err.file}: ${err.error}`));
              }

            } catch (error) {
              console.error('üí• Fatal error during sync:', error);
              process.exit(1);
            }
          }

          async processBlog(filePath) {
            console.log(`üìñ Processing: ${filePath}`);
            
            const blogFolder = path.dirname(filePath).split('/').pop();
            const blogFolderPath = path.dirname(filePath);
            
            const content = await fs.readFile(filePath, 'utf-8');
            const parsed = frontMatter(content);
            
            const metadata = this.extractMetadata(parsed, blogFolder);
            const processedContent = await this.processAssets(parsed.body, blogFolderPath, blogFolder);
            
            await this.upsertBlog(metadata, processedContent, blogFolder);
            
            this.processedBlogs.push({
              title: metadata.title,
              slug: blogFolder,
              folder: blogFolderPath
            });
          }

          extractMetadata(parsed, blogFolder) {
            const { attributes, body } = parsed;
            
            let title = attributes.title;
            if (!title) {
              const titleMatch = body.match(/^#\s+(.+)$/m);
              title = titleMatch ? titleMatch[1] : blogFolder.replace(/-/g, ' ');
            }
            
            let excerpt = attributes.excerpt || attributes.description;
            if (!excerpt) {
              const paragraphMatch = body.match(/^(?!#)(.+)$/m);
              excerpt = paragraphMatch ? paragraphMatch[1].substring(0, 200) + '...' : '';
            }
            
            const wordCount = body.split(/\s+/).length;
            const readTime = Math.ceil(wordCount / 200);
            
            return {
              title: title.trim(),
              excerpt: excerpt.trim(),
              tags: attributes.tags || attributes.categories || [],
              featured: attributes.featured || false,
              date: attributes.date || new Date().toISOString().split('T')[0],
              read_time: `${readTime} min read`,
              status: attributes.status || 'published'
            };
          }

          async processAssets(content, blogFolderPath, blogFolder) {
            let processedContent = content;
            
            const imageRegex = /!\[([^\]]*)\]\(([^)]+)\)/g;
            let match;
            
            while ((match = imageRegex.exec(content)) !== null) {
              const [fullMatch, altText, imagePath] = match;
              
              if (imagePath.startsWith('http')) continue;
              
              try {
                const githubAssetUrl = `${GITHUB_RAW_BASE}/blogs/${blogFolder}/${imagePath}`;
                
                processedContent = processedContent.replace(fullMatch, 
                  `![${altText}](${githubAssetUrl})`);
                console.log(`üñºÔ∏è  Processed asset: ${imagePath}`);
              } catch (error) {
                console.warn(`‚ö†Ô∏è  Could not process asset ${imagePath}:`, error.message);
              }
            }
            
            return processedContent;
          }

          async upsertBlog(metadata, content, slug) {
            try {
              const { data: existing } = await supabase
                .from('blogs')
                .select('id')
                .eq('slug', slug)
                .single();

              const blogData = {
                title: metadata.title,
                excerpt: metadata.excerpt,
                content: content,
                date: metadata.date,
                read_time: metadata.read_time,
                tags: JSON.stringify(metadata.tags),
                featured: metadata.featured,
                slug: slug,
                status: metadata.status,
                github_synced: true,
                github_folder: slug,
                updated_at: new Date().toISOString()
              };

              if (existing) {
                const { error } = await supabase
                  .from('blogs')
                  .update(blogData)
                  .eq('id', existing.id);
                
                if (error) throw error;
                console.log(`‚úÖ Updated blog: ${metadata.title}`);
              } else {
                const { error } = await supabase
                  .from('blogs')
                  .insert([blogData]);
                
                if (error) throw error;
                console.log(`üÜï Created new blog: ${metadata.title}`);
              }
            } catch (error) {
              throw new Error(`Failed to upsert blog ${slug}: ${error.message}`);
            }
          }
        }

        async function main() {
          const sync = new GitHubBlogSync();
          await sync.syncAllBlogs();
        }

        main().catch(console.error);
        EOF
        
    - name: Run blog sync
      run: node sync-blogs.js
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        GITHUB_REPOSITORY: ${{ github.repository }}
